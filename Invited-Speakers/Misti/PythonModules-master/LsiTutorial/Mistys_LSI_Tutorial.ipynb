{
 "metadata": {
  "name": "Mistys_LSI_Tutorial"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Preface\n",
      "\n",
      "Using free text in machine learning applications presents many problems. Text is inherently messy. It cannot easily be transformed into a numeric or a factor representation, which makes it difficult to look at training cases with large swaths of text and feed it into a machine learning algorithm.\n",
      "\n",
      "In this tutorial, we will show a method for using Latent Semantic Indexing (LSI) in the gensim package of Python to turn messy text into tidy features. These features represent the similarity between the document and the most significant topics gleaned from the text in all the documents. The use of features generated from LSI in a machine learning application leads to learning that is more accurate than traditional Term Frequency or TF-IDF models.\n",
      "\n",
      "This tutorial was written and tested in Python 2.7. YMMV in other versions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Preparing the test data to run the code in this notebook:\n",
      "\n",
      "1. Download the training data set from the Kaggle Salary competition [Train_rev1.csv](https://www.kaggle.com/c/job-salary-prediction/data), and place it into the \"data\" subdirectory.\n",
      "2. From a terminal, unzip the training data \n",
      "\n",
      "        sh> cd data\n",
      "        sh> unzip Train_rev1.zip\n",
      "        Archive:  Train_rev1.zip\n",
      "          inflating: Train_rev1.csv\n",
      "        sh> head -n 1 Train_rev1.csv\n",
      "        Id,Title,FullDescription,LocationRaw,...,Category, ...\n",
      "        sh> cd ..\n",
      "\n",
      "3. Using the utility provided in the dictionary, select out only the rows that have the category \"IT Jobs\"\n",
      "\n",
      "        sh> python get_IT_train.py\n",
      "\n",
      "4. You should now have a file named \"Train_IT.csv\" in your data directory. Each line in this file is the FullDescription of a job in the \"IT Jobs\" category."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Setting Up Your Environment\n",
      "\n",
      "Required import statements, both external and code in this directory. Note that, with each code segment \n",
      "in this notebook, I will be including import statements for the packages used within that code snippet.\n",
      "These statements are informational, and should execute basically as a no-op."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "from gensim import corpora, models, similarities, utils\n",
      "import pandas as pd\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# My imports\n",
      "import data_io"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data_io module in this directory contains useful functions related to filenames and to reading and writing the different files used by this code. It relies on a file \"SETTINGS.json\" that contains a json mapping from variables to file name components.\n",
      "\n",
      "For example, you can use the data_io module to read in stop words from a file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist = data_io.get_stopwords_list()\n",
      "print stoplist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['a', 'an', 'and', 'hello', 'hi', 'i', 'or', 'our', 'thank', 'that', 'the', 'them', 'they', 'their', 'this', 'you', 'your', 'we']\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Creating a Dictionary from the Corpus\n",
      "\n",
      "The first step in a bag-of-words approach for natural language processing is to read in the different text fields and tokenize their contents. The tokens will then serve as a basis for a dictionary of all the tokens in the corpus. The corpus is the set of strings you are considering in your NLP application.\n",
      "\n",
      "Here, our documents are represented as csv files, and our corpus is just considering a single field (\"fieldnum\") within each record of the csv file."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a standard method for preprocessing all text. This can be whatever you want, but here we will simply use the simple preprocessor from gensim."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import utils\n",
      "\n",
      "def tokenize(text, stoplist):\n",
      "    ''' The standard method that will be used for preprocessing all text.\n",
      "        The words from the stoplist will be removed from the token stream.\n",
      "    '''\n",
      "    # Use the gensim simple preprocessor to parse each line\n",
      "    tokens = utils.simple_preprocess(text)\n",
      "    return [token for token in tokens if not token in stoplist]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tokenize(\"We currently operate over 5000 websites. We are looking for a system administrator \\\n",
      "    to manage the servers.\", stoplist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['currently', 'operate', 'over', 'websites', 'are', 'looking', 'for', 'system', 'administrator', 'to', 'manage', 'servers']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to generate a dictionary containing all the tokens from the field being indexed for each document \n",
      "in the corpus. In gensim, the dictionary for a token contains a \"tokenid\", which is a unique index for the token.\n",
      "It also contains a \"docfreq\", "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora\n",
      "\n",
      "def createDictionary(input_filename, stoplist):\n",
      "    ''' The dictionary contains statistics about all the tokens in the corpus. This code\n",
      "        does not include in the dictionary either stopwords or words that appear only once\n",
      "        in the corpus.\n",
      "    '''\n",
      "    dictionary = corpora.Dictionary(tokenize(line, stoplist) for line in open(input_filename))\n",
      "\n",
      "    # Remove words that appear only once\n",
      "    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
      "    dictionary.filter_tokens(once_ids)\n",
      "    dictionary.compactify() # remove gaps in word ids\n",
      "\n",
      "    # print dictionary.token2id\n",
      "    return dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_filename = \"./data/Train_IT.csv\"\n",
      "corpus_dictionary = createDictionary(corpus_filename, stoplist)\n",
      "\n",
      "print \"First ten tokens in the dictionary\"\n",
      "print corpus_dictionary.values()[:10]\n",
      "\n",
      "print \"\\nIndex of the 'websites' token in the dictionary\"\n",
      "print corpus_dictionary.token2id['websites']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:43:09,047 : INFO : adding document #0 to Dictionary(0 unique tokens)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:43:22,589 : INFO : adding document #10000 to Dictionary(26113 unique tokens)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:43:35,512 : INFO : adding document #20000 to Dictionary(33609 unique tokens)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:43:48,572 : INFO : adding document #30000 to Dictionary(39479 unique tokens)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:43:59,444 : INFO : built Dictionary(42954 unique tokens) from 38483 documents (total 8294889 corpus positions)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "First ten tokens in the dictionary\n",
        "['candidents', 'circuitry', 'francesca', 'multisourcing', 'originality', 'marketled', 'xunit', 'kaspersky', 'loughton', 'replaces']\n",
        "\n",
        "Index of the 'websites' token in the dictionary\n",
        "5712\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the bag-of-words approach to NLP, the basis of the bag of words is a term vector, which counts each \n",
      "(non-stopword) term in a string. The returned bag-of-words vector for a string has, for each unique word, \n",
      "an element containing the index of the word in the dictionary and a count of the number of times that\n",
      "word appears in the string."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_doc = \"We currently operate over 5000 websites. We are looking for a system administrator to manage the servers.\" \n",
      "print test_doc\n",
      "\n",
      "tokens = tokenize(test_doc, stoplist)\n",
      "print \"Tokens are: \" + str(tokens)\n",
      "\n",
      "test_vec = corpus_dictionary.doc2bow(tokens)  # Creates the bag-of-words for test_doc\n",
      "print \"Bag of words is: \" + str(test_vec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "We currently operate over 5000 websites. We are looking for a system administrator to manage the servers.\n",
        "Tokens are: ['currently', 'operate', 'over', 'websites', 'are', 'looking', 'for', 'system', 'administrator', 'to', 'manage', 'servers']\n",
        "Bag of words is: [(2934, 1), (3402, 1), (5712, 1), (6896, 1), (12279, 1), (14326, 1), (14928, 1), (15743, 1), (21330, 1), (21475, 1), (25093, 1), (25496, 1)]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the dictionary contains only the words that were in the original corpus. In particular, new documents\n",
      "may contain worsare not in the corpus. The new document in the example below contains words not in the dictionary,\n",
      "so we see that there are more tokens than there are entries in the bag of words.\n",
      "\n",
      "This points out a limitation of our bag-of-words approach -- the dictionary\n",
      "may be incomplete with respect to words in new documents. This can be remedied either by augmenting your\n",
      "dictionary or by using a hashing approach, but we will not do so at this time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_doc2 = \"The quick brown fox jumped over the lazy dog.\"\n",
      "print \"\\n\" + test_doc2\n",
      "\n",
      "tokens2 = tokenize(test_doc2, stoplist)\n",
      "print \"Tokens are \" + str(tokens2)\n",
      "\n",
      "test_vec2 = corpus_dictionary.doc2bow(tokens2)\n",
      "print \"Bag of words is: \" + str(test_vec2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The quick brown fox jumped over the lazy dog.\n",
        "Tokens are ['quick', 'brown', 'fox', 'jumped', 'over', 'lazy', 'dog']\n",
        "Bag of words is: [(6898, 1), (17111, 1), (22415, 1), (25496, 1), (25828, 1)]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that some of the tokens are common words that we expect to see in most of the documents in the corpus, \n",
      "such as the word \"are\". Others are rarer, and more indicative of what the document is really talking about, \n",
      "such as the word \"system\". The TF-IDF approach takes the bag-of-words (the \"term vector\"), and discounts\n",
      "each term based on how many of the documents in the corpus the term appeared in. You can find a good explanation\n",
      "of TF-IDF in [Wikipedia](http://en.wikipedia.org/wiki/Tf%E2%80%93idf). \n",
      "\n",
      "Notice that the TF-IDF representation of a string may change as the corpus changes, both because the set \n",
      "of words in the corpus will change, and because the document frequencies for a word will change.\n",
      "\n",
      "Here, we show our test bag of words turned into a TF-IDF vector. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import models\n",
      "\n",
      "def getBows(filename, dictionary, stoplist):\n",
      "    corpus = [tokenize(line, stoplist) for line in open(filename)]\n",
      "    return [dictionary.doc2bow(text) for text in corpus]\n",
      "\n",
      "def getTfidfModel(corpus_bows):\n",
      "    ''' Make TFIDF model for the field represented in the corpus.\n",
      "    '''\n",
      "    return models.TfidfModel(corpus_bows)\n",
      "\n",
      "# Build the TF-IDF model\n",
      "corpus_filename = \"./data/Train_IT.csv\"\n",
      "corpus_bows = getBows(corpus_filename, corpus_dictionary, stoplist)\n",
      "corpus_tfidf = getTfidfModel(corpus_bows)\n",
      "print corpus_tfidf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:14,186 : INFO : collecting document frequencies\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:14,187 : INFO : PROGRESS: processing document #0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:14,573 : INFO : PROGRESS: processing document #10000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:14,947 : INFO : PROGRESS: processing document #20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:15,323 : INFO : PROGRESS: processing document #30000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:15,654 : INFO : calculating IDF weights for 38483 documents and 27071 features (5230347 matrix non-zeros)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TfidfModel(num_docs=38483, num_nnz=5230347)\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now translate a new document or set of docs into same tfidf using res=tfidf[docs]. \n",
      "\n",
      "In the test below, you can see the different steps in the transformation of the test_doc string to its TF-IDF representation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Test doc is: \" + test_doc\n",
      "print \"\\nTokens are: \" + str(tokens)\n",
      "print \"\\nBag of words is: \" + str(test_vec)\n",
      "print \"\\nTF-IDF is: \" + str(corpus_tfidf[test_vec])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test doc is: We currently operate over 5000 websites. We are looking for a system administrator to manage the servers.\n",
        "\n",
        "Tokens are: ['currently', 'operate', 'over', 'websites', 'are', 'looking', 'for', 'system', 'administrator', 'to', 'manage', 'servers']\n",
        "\n",
        "Bag of words is: [(2934, 1), (3402, 1), (5712, 1), (6896, 1), (12279, 1), (14326, 1), (14928, 1), (15743, 1), (21330, 1), (21475, 1), (25093, 1), (25496, 1)]\n",
        "\n",
        "TF-IDF is: [(2934, 0.27115705629990106), (3402, 0.4310977880234736), (5712, 0.41159922073979927), (6896, 0.0072369611296590215), (12279, 0.048862490250979364), (14326, 0.001493196488406858), (14928, 0.2365989206947037), (15743, 0.11389168799069467), (21330, 0.24395103555494677), (21475, 0.4078999453125804), (25093, 0.4274056850890632), (25496, 0.3020795005006154)]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TF-IDF considers a document based on the individual terms in the document, how frequent they are, and how distinctive they might be to a particular class of document. However, documents usually focus on specific *topics*, where each topic is characterized by a collection of vocabulary, or a set of phrases. For instance, you might see the words [\"programmer\", \"linux\", \"code\", \"python\", and \"system\"] in a job advertisement for a programmer, and [\"system\", \"administrator\", \"linux\"] in a job advertisement for a sysadmin. Furthermore, there may be analogous or synonymous words that would occur in one context rather than another. For example, a programming job could easily have the words [\"C/C++\", \"python\", \"java\"] serving a similar function, but none of these words is likely to appear in a job advertisement for a sysadmin.\n",
      "\n",
      "Latent Semantic Analysis (or Latent Semantic Indexing) is one method used to find topics in a corpus of documents. LSI can also be used to see how similar other documents not in the corpus are to the topics identified by the corpus.\n",
      "\n",
      "For more information on how LSI works, you can look at the [Wikipedia article on LSI](http://en.wikipedia.org/wiki/Latent_semantic_indexing).\n",
      "\n",
      "Here, we will show how to use gensim to extract lsi topics from the tf-idf representation of the corpus documents. So the computation sequence here for the different representations of a document is \n",
      "\n",
      "    text --> bag of words --> tfidf --> lsi\n",
      "\n",
      "One other point we illustrate here is the restriction of the number of topics computed to a subset of the topics that are deemed most significant. In this example, we restrict the number of topics selected to 200 topics. In practice, the selection of the number of topics kept can have an effect on the accuracy of the systems using the LSI vectors. Selecting too few topics results in the documents being under-characterized because important information is lost. Selecting too many topics results in the characterization of the document including spurious topics or noise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import models\n",
      "\n",
      "num_topics=200\n",
      "\n",
      "def getLSIModel(bows, corpus_tfidf, dictionary, topic_count):\n",
      "    ''' Make LSI model for the field represented in the corpus, based on its tfidf.\n",
      "        Can translate a set of docs into same tfidf using res=lsi[tfidf[[docs]]\n",
      "    '''\n",
      "    return models.LsiModel(corpus_tfidf[bows], id2word=dictionary, num_topics=topic_count)\n",
      "\n",
      "corpus_lsi = getLSIModel(corpus_bows, corpus_tfidf, corpus_dictionary, num_topics)\n",
      "print corpus_lsi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:36,723 : INFO : using serial LSI version on this node\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:36,724 : INFO : updating model with new documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:41,199 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:42,452 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:42,535 : INFO : 1st phase: constructing (27071, 300) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:43,861 : INFO : orthonormalizing (27071, 300) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:52,429 : INFO : 2nd phase: running dense svd on (300, 20000) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:54,688 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:54,723 : INFO : keeping 200 factors (discarding 12.653% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:54,933 : INFO : processed documents up to #20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:54,941 : INFO : topic #0(28.305): 0.205*\"net\" + 0.140*\"developer\" + 0.119*\"software\" + 0.113*\"asp\" + 0.112*\"web\" + 0.107*\"sql\" + 0.100*\"support\" + 0.093*\"development\" + 0.091*\"server\" + 0.090*\"project\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:54,944 : INFO : topic #1(16.121): -0.578*\"net\" + -0.344*\"asp\" + -0.236*\"developer\" + -0.150*\"web\" + -0.147*\"mvc\" + -0.133*\"vb\" + -0.124*\"javascript\" + -0.118*\"css\" + -0.117*\"html\" + 0.114*\"support\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:54,947 : INFO : topic #2(14.173): 0.634*\"allegis\" + 0.400*\"group\" + 0.241*\"applicant\" + 0.152*\"fulfilment\" + 0.146*\"limited\" + 0.138*\"companies\" + 0.135*\"data\" + 0.123*\"privacy\" + 0.096*\"such\" + 0.084*\"teksystems\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:54,950 : INFO : topic #3(12.411): 0.420*\"php\" + -0.393*\"net\" + 0.215*\"java\" + -0.208*\"asp\" + 0.167*\"html\" + 0.165*\"css\" + 0.156*\"web\" + 0.151*\"javascript\" + -0.150*\"server\" + 0.149*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:54,953 : INFO : topic #4(11.281): 0.271*\"php\" + -0.260*\"test\" + 0.209*\"windows\" + 0.182*\"support\" + -0.173*\"testing\" + 0.159*\"network\" + -0.153*\"project\" + 0.139*\"line\" + 0.138*\"infrastructure\" + 0.129*\"directory\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:45:59,283 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:00,422 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:00,448 : INFO : 1st phase: constructing (27071, 300) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:01,593 : INFO : orthonormalizing (27071, 300) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:09,786 : INFO : 2nd phase: running dense svd on (300, 18483) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:11,762 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:11,762 : INFO : keeping 200 factors (discarding 12.371% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:11,925 : INFO : merging projections: (27071, 200) + (27071, 200)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:13,031 : INFO : keeping 200 factors (discarding 5.975% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:13,302 : INFO : processed documents up to #38483\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:13,306 : INFO : topic #0(39.440): 0.227*\"net\" + 0.142*\"developer\" + 0.122*\"asp\" + 0.119*\"software\" + 0.113*\"web\" + 0.110*\"sql\" + 0.099*\"support\" + 0.096*\"server\" + 0.093*\"development\" + 0.089*\"project\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:13,309 : INFO : topic #1(23.135): -0.610*\"net\" + -0.349*\"asp\" + -0.215*\"developer\" + -0.150*\"mvc\" + -0.144*\"vb\" + -0.132*\"web\" + 0.108*\"support\" + -0.104*\"html\" + -0.103*\"css\" + -0.103*\"javascript\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:13,312 : INFO : topic #2(19.521): -0.630*\"allegis\" + -0.400*\"group\" + -0.239*\"applicant\" + -0.151*\"fulfilment\" + -0.146*\"limited\" + -0.137*\"companies\" + -0.133*\"data\" + -0.121*\"privacy\" + -0.097*\"such\" + -0.083*\"teksystems\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:13,315 : INFO : topic #3(17.349): 0.391*\"php\" + -0.344*\"net\" + 0.224*\"java\" + 0.176*\"css\" + 0.176*\"html\" + 0.166*\"web\" + -0.165*\"asp\" + 0.163*\"javascript\" + -0.143*\"server\" + 0.143*\"mysql\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:46:13,318 : INFO : topic #4(15.704): 0.266*\"php\" + -0.216*\"test\" + 0.209*\"windows\" + 0.185*\"support\" + -0.166*\"project\" + 0.157*\"network\" + 0.154*\"line\" + -0.142*\"testing\" + 0.134*\"engineer\" + 0.133*\"infrastructure\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LsiModel(num_terms=27071, num_topics=200, decay=1.0, chunksize=20000)\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So now, if you look at a single topic, you will see that it is comprised of a set of words from the dictionary, each weighted according to how it is to the topic. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_lsi.print_topics(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2013-09-11 12:47:37,444 : INFO : topic #0(39.440): 0.227*\"net\" + 0.142*\"developer\" + 0.122*\"asp\" + 0.119*\"software\" + 0.113*\"web\" + 0.110*\"sql\" + 0.099*\"support\" + 0.096*\"server\" + 0.093*\"development\" + 0.089*\"project\"\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "['0.227*\"net\" + 0.142*\"developer\" + 0.122*\"asp\" + 0.119*\"software\" + 0.113*\"web\" + 0.110*\"sql\" + 0.099*\"support\" + 0.096*\"server\" + 0.093*\"development\" + 0.089*\"project\"']"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have a dictionary of terms, a TF-IDF representation of the corpus, and an LSI representation over that TF-IDF representation, we can use the TF-IDF and LSI models in tandem to compute, for a new document, how similar it is to each of the topics in the corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def text2lsi(text, dictionary, stoplist, corpus_tfidf, corpus_lsi):\n",
      "    return corpus_lsi[corpus_tfidf[dictionary.doc2bow(tokenize(text, stoplist))]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can look at a couple of new documents and see how well they fit into the existing topics. If we look at topic 0, below, the category appears to include server programmers in the .net world. Test_doc1, which is an advertisement in the same space, should show a strong similarity to topic 0, which it does.\n",
      "\n",
      "Topic 8, on the other hand, is looking for Jaa programmers, but definitely not for qa or software testing. If we look at the document test_doc2, which is advertising for a tester, we would expect to see a strong negative correlation with topic 8, which we do.\n",
      "\n",
      "Note that LSI characterizes a document in terms of its similarity to each one of the topics. Note how dissimilar test_doc1 is to topic 1, which seems to be more centered around web development than backend servers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Topic 0 is: \" + str(corpus_lsi.show_topic(0))\n",
      "print \"\\nTopic 1 is: \" + str(corpus_lsi.show_topic(1))\n",
      "print \"\\nTopic 8 is: \" + str(corpus_lsi.show_topic(8))\n",
      "\n",
      "test_doc1 = \"Company XYZ is actively seeking .net software developers to work on our backend servers. \\\n",
      "             Proficiency in SQL is a requirement.\"\n",
      "print \"\\nTest doc 1 is: \" + test_doc1\n",
      "print \"\\nFirst few LSI topics are: \" + str(text2lsi(test_doc1, corpus_dictionary, stoplist, corpus_tfidf, corpus_lsi)[:10])\n",
      "\n",
      "test_doc2 = \"We are looking for a tester for our qa department to work in test automation.\"\n",
      "\n",
      "print \"\\nTest doc 2 is: \" + test_doc2\n",
      "print \"\\nFirst few LSI topics are: \" + str(text2lsi(test_doc2, corpus_dictionary, stoplist, corpus_tfidf, corpus_lsi)[:10])\n",
      "\n",
      "test_doc3 = \"The elephant sighed.\"\n",
      "\n",
      "print \"\\nTest doc 3 is: \" + test_doc3\n",
      "topics3 = text2lsi(test_doc3, corpus_dictionary, stoplist, corpus_tfidf, corpus_lsi)\n",
      "print \"\\nFirst few LSI topics are: \" + str(topics3[:10])\n",
      "print len(topics3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Topic 0 is: [(0.22658247961774214, 'net'), (0.14217706531745913, 'developer'), (0.12233298723880021, 'asp'), (0.11933644745404097, 'software'), (0.1127006077824269, 'web'), (0.10985994247471542, 'sql'), (0.098789561592898978, 'support'), (0.095988880759862671, 'server'), (0.093099647570403912, 'development'), (0.088538862954378864, 'project')]\n",
        "\n",
        "Topic 1 is: [(-0.60969856441699177, 'net'), (-0.34865460178645202, 'asp'), (-0.215017753201689, 'developer'), (-0.1501657488503185, 'mvc'), (-0.14410786775161283, 'vb'), (-0.1321907666658049, 'web'), (0.10841452152596809, 'support'), (-0.10408823443203541, 'html'), (-0.10328874163290516, 'css'), (-0.10328057492771725, 'javascript')]\n",
        "\n",
        "Topic 8 is: [(0.5283846779323621, 'java'), (-0.38926892172780558, 'test'), (-0.29372350494416583, 'php'), (-0.24668457987328199, 'testing'), (0.18965003097406222, 'sales'), (0.17713071958470078, 'ee'), (0.14412509907533305, 'spring'), (-0.11960969387935132, 'tester'), (-0.11864783170254123, 'qa'), (-0.10474016962504196, 'automation')]\n",
        "\n",
        "Test doc 1 is: Company XYZ is actively seeking .net software developers to work on our backend servers.              Proficiency in SQL is a requirement.\n",
        "\n",
        "First few LSI topics are: [(0, 0.11720699597700321), (1, -0.12535131799860852), (2, 0.0061035037275637945), (3, -0.057809160080476422), (4, -0.00069377295584700945), (5, 0.024425835540166425), (6, 0.024809079774395427), (7, -0.011191452498555946), (8, 0.0034102613093680723), (9, 0.017520089968137014)]\n",
        "\n",
        "Test doc 2 is: We are looking for a tester for our qa department to work in test automation.\n",
        "\n",
        "First few LSI topics are: [(0, 0.06711749799585319), (1, 0.033541535637204578), (2, -0.0039666836794347085), (3, 0.061456356064917485), (4, -0.14753349585399156), (5, 0.20426253966250699), (6, 0.18110762825251689), (7, 0.19455597658879015), (8, -0.28223444566785955), (9, 0.065030276829440875)]\n",
        "\n",
        "Test doc 3 is: The elephant sighed.\n",
        "\n",
        "First few LSI topics are: []\n",
        "0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/mistynodine/anaconda/lib/python2.7/site-packages/scipy/sparse/compressed.py:122: UserWarning: indices array has non-integer dtype (float64)\n",
        "  % self.indices.dtype.name )\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The only thing that remains, then, to making input document representations tidy, so they can be fed into a Machine Learning application, is to convert the returned LSI topics into a fixed-width array, with one column per topic and one row per document. Note that in some cases, a drastic one of which is shown in test doc 3 (above), the document has a zero correlation with some or all of the topics.\n",
      "\n",
      "First, this code converts the lsi topics for a single document in the corpus into a fixed-width vector of floats."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def text2lsi_tidy(text, dictionary, stoplist, corpus_tfidf, corpus_lsi, num_features):\n",
      "    text_features = [0] * num_features\n",
      "    text_lsi = corpus_lsi[corpus_tfidf[dictionary.doc2bow(tokenize(text, stoplist))]]\n",
      "    if text_lsi != []:\n",
      "        for t in text_lsi:\n",
      "            text_features[t[0]] = t[1] \n",
      "    return text_features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Test doc 1 is: \" + test_doc1\n",
      "text_lsi_tidy1 = text2lsi_tidy(test_doc1, corpus_dictionary, stoplist, corpus_tfidf, corpus_lsi, num_topics)\n",
      "print \"LSI vector begins with: \" + str(text_lsi_tidy1[:10])\n",
      "print \"Vector length is: \" + str(len(text_lsi_tidy1))\n",
      "\n",
      "print \"\\nTest doc 3 is: \" + test_doc3\n",
      "text_lsi_tidy3 = text2lsi_tidy(test_doc3, corpus_dictionary, stoplist, corpus_tfidf, corpus_lsi, num_topics)\n",
      "print \"LSI vector begins with: \" + str(text_lsi_tidy3[:10])\n",
      "print \"Vector length is: \" + str(len(text_lsi_tidy3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test doc 1 is: Company XYZ is actively seeking .net software developers to work on our backend servers.              Proficiency in SQL is a requirement.\n",
        "LSI vector begins with: [0.11720699597700321, -0.12535131799860852, 0.0061035037275637945, -0.057809160080476422, -0.00069377295584700945, 0.024425835540166425, 0.024809079774395427, -0.011191452498555946, 0.0034102613093680723, 0.017520089968137014]\n",
        "Vector length is: 200\n",
        "\n",
        "Test doc 3 is: The elephant sighed.\n",
        "LSI vector begins with: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "Vector length is: 200\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def docs2lsi_tidy(doclist, dictionary, stoplist, tfidf, lsi, num_features):\n",
      "  lsi_features = [text2lsi_tidy(d, dictionary, stoplist, tfidf, lsi, num_features) \\\n",
      "                  for d in doclist]\n",
      "  return np.array(lsi_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can use this to make an array with one row per document, and one column per topic."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = [test_doc1,test_doc2]\n",
      "print \"Docs are: \" + str(docs)\n",
      "res = docs2lsi_tidy(docs, corpus_dictionary, stoplist, corpus_tfidf, corpus_lsi, num_topics)\n",
      "print \"\\nShape of the resulting array is: \" + str(res.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Docs are: ['Company XYZ is actively seeking .net software developers to work on our backend servers.              Proficiency in SQL is a requirement.', 'We are looking for a tester for our qa department to work in test automation.']\n",
        "\n",
        "Shape of the resulting array is: (2, 200)\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}